{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prerequisite:\n",
    "\n",
    "`%pip install word2word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"de2tr\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanup(name:str,lang:str=\"en\"):\n",
    "\n",
    "    raw_content = open(f\"../static/books/{name}/{lang}.raw\", \"r\").read()\n",
    "\n",
    "    raw_content = raw_content.replace(\"\\n\",\" \")\n",
    "    raw_content = raw_content.replace(\"Mr.\",\"Mr<dot>\")\n",
    "    raw_content = raw_content.replace(\"Mrs.\",\"Mrs<dot>\")\n",
    "    raw_content = raw_content.replace(\". .\",\"..\")\n",
    "    raw_content = raw_content.replace(\". .\",\"..\")\n",
    "    raw_content = raw_content.replace(\"...\",\"…\")\n",
    "\n",
    "    raw_content = raw_content.replace(\". \",\".\\n\")\n",
    "    raw_content = raw_content.replace(\"? \",\"?\\n\")\n",
    "    raw_content = raw_content.replace(\"! \",\"!\\n\")\n",
    "    raw_content = raw_content.replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "    raw_content = raw_content.replace(\"\\n\\\"\\n\",\"\\\"\\n\")\n",
    "\n",
    "\n",
    "    raw_content = raw_content.replace(\"<dot>\",\".\")\n",
    "\n",
    "    lines = raw_content.split(\"\\n\")\n",
    "\n",
    "    lines = map(lambda x: x.strip(), lines)\n",
    "    lines = map(lambda x: x.strip(\"\\n\"), lines)\n",
    "    \n",
    "    parts = [\"\"]\n",
    "\n",
    "    min_content = 100\n",
    "\n",
    "    max_content = 200\n",
    "\n",
    "    # for line in lines:\n",
    "    #     if len (parts[-1]) < min_content and len(parts[-1] + line) < max_content: \n",
    "    #         parts[-1] += \"\\n\" + line\n",
    "    #     else:\n",
    "    #         parts.append(line)\n",
    "\n",
    "    parts = list(lines)\n",
    "\n",
    "    parts[0] = parts[0].strip(\"\\n\")\n",
    "    \n",
    "    print(\"num parts\",len(parts))\n",
    "    # longest part \n",
    "    print(\"longest part\",max([len(part) for part in parts]))\n",
    "\n",
    "\n",
    "    # json.dump(parts, open(f\"../static/books/{name}/{lang}.json\", \"w\"),indent=4)\n",
    "\n",
    "    with open(f\"../static/books/{name}/{lang}.lines\", \"w\") as outfile:\n",
    "        for line in parts:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# cleanup(\"the_little_prince\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num parts 101\n",
      "longest part 691\n"
     ]
    }
   ],
   "source": [
    "cleanup(\"mobydick\",\"en\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## book collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookCollection:\n",
    "    def __init__(self, data:dict[str,dict[str,list[str]]] = {}):\n",
    "        self.data = data\n",
    "    \n",
    "    def add_book(self, lang:str,title:str, txt:list[str]):\n",
    "        if lang in self.data:\n",
    "            self.data[lang][title] = txt\n",
    "        else:\n",
    "            self.data[lang] = {title:txt}\n",
    "\n",
    "    def get_book(self, lang:str, title:str):\n",
    "\n",
    "        if lang in self.data and title in self.data[lang]:\n",
    "            return self.data[lang][title]\n",
    "\n",
    "        else:\n",
    "            \n",
    "            dir = f\"../static/books/{title}/{lang}.lines\"\n",
    "            if os.path.exists(dir):\n",
    "                book = []\n",
    "                for line in open(dir,\"r\"):\n",
    "                    line = line.strip()\n",
    "                    if len(line) > 0:\n",
    "                        book.append(line)\n",
    "                self.add_book(lang,title,book)\n",
    "                return book\n",
    "            else:\n",
    "                print(\"no such book\",dir)\n",
    "                return None\n",
    "    def remove_book (self, lang:str, title:str):\n",
    "        if lang in self.data and title in self.data[lang]:\n",
    "            del self.data[lang][title]\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "bookcollection = BookCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_text = bookcollection.get_book(\"de\",\"mobydick\")\n",
    "target_text = bookcollection.get_book(\"tr\",\"mobydick\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = {\n",
    "    \"de2ru\": '''Ich versuche russisch zu lernen, indem ich ein Buch lese. Der Originaltext lautet [{}] die deutsche Version lautet [{}]\n",
    "finde alle deutschen Wörter und die passende russische Übersetzung aus dem Text und gib sie im JSON-Format an, wie hier: [[\"Apfel\",\"яблоко\"],[\"essen\", \"есть\"]]\n",
    "KI: Hier ist eine Liste von Übersetzungen im JSON-Format: [[\"''',\n",
    "\n",
    "    \"de2tr\": '''Ich versuche türkisch zu lernen, indem ich ein Buch lese. Der Originaltext lautet [{}] die deutsche Version lautet [{}]\n",
    "finde alle deutschen Wörter und die passende türkische Übersetzung aus dem Text und gib sie im JSON-Format an, wie hier: [[\"Apfel\",\"elma\"],[\"essen\", \"yemek\"]]\n",
    "KI: Hier ist eine Liste von Übersetzungen im JSON-Format: [[\"''',\n",
    "\n",
    "    \"en2de\": '''I am trying to learn german by reading a book. the original text is [{}] the german version is [{}]\n",
    "translate the german words and put the translations for single words in json format like this: [[\"apple\", \"Apfel\"],[\"eat\", \"essen\"]]\n",
    "AI: here is a list of translations: [[\"'''\n",
    "}\n",
    "\n",
    "def get_translations(origin_sentence,translation_sentence,lang=\"en2de\"):\n",
    "\n",
    "    prompt = prompts[lang].format(origin_sentence,translation_sentence)\n",
    "\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages= [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens = 500,\n",
    "        stop = \"]]\"\n",
    "    )\n",
    "\n",
    "    txt = resp.choices[0].message.content\n",
    "    answer = '[[\"'+txt + \"]]\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor einigen Jahren - ich weiß nicht, wie lange genau - hatte ich wenig oder gar kein Geld in der Tasche und nichts Besonderes, was mich an Land interessierte, und so dachte ich, ich würde ein wenig herumsegeln und den wässrigen Teil der Welt sehen.\n",
      "\n",
      "Birkaç yıl önce -tam olarak ne kadar zaman olduğu önemli değil- cüzdanımda çok az para olduğu ya da hiç para olmadığı ve kıyıda ilgimi çekecek özel bir şey olmadığı için biraz yelken açıp dünyanın su kısmını görmeyi düşündüm.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[\"Jahre\",\"yıl\"],[\"wenig\",\"az\"],[\"gar kein\",\"hiç\"],[\"Geld\",\"para\"],[\"Tasche\",\"cüzdan\"],[\"Besonderes\",\"özel bir şey\"],[\"Land\",\"kıyı\"],[\"herumsegeln\",\"yelken açmak\"],[\"wässrig\",\"sulu\"],[\"Teil\",\"kısım\"],[\"Welt\",\"dünya\"]]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i=1\n",
    "# get_translations(en_text[i],de_text[i])\n",
    "\n",
    "print(origin_text[i])\n",
    "print()\n",
    "print(target_text[i])\n",
    "print()\n",
    "\n",
    "get_translations(origin_text[i], target_text[i],lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating 100 sentences\n",
      "..........9\n",
      "..........19\n",
      "..........29\n",
      "....That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 93c699f559b03768f91e132fd68e3faa in your message.)\n",
      "......39\n",
      "..........49\n",
      ".......That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 4ab7025dc0fbb9231e4f6c03c753fcb4 in your message.)\n",
      "..That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID e50fcb56a577c192b28f4dee1a2d7e56 in your message.)\n",
      ".59\n",
      ".........That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID b943019d6a7f6c58ede025159166d467 in your message.)\n",
      ".69\n",
      "..........79\n",
      "..........89\n",
      "......That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID a3ca0b5b69780f76a128464c8a926c09 in your message.)\n",
      "....99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n = len(origin_text)\n",
    "\n",
    "print(f\"translating {n} sentences\")\n",
    "for i in range(len(translations),n):\n",
    "\n",
    "    params = [origin_text[i],target_text[i],lang]\n",
    "    try:\n",
    "        new_t = get_translations(*params)\n",
    "        translations.append(new_t)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        # retry once\n",
    "        try:\n",
    "            new_t = get_translations(*params)\n",
    "            translations.append(new_t)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            translations.append(\"<error>\")\n",
    "    \n",
    "    print(\".\",end=\"\")\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tranlations.json\", \"w\") as f:\n",
    "    json.dump(translations,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor einigen Jahren - ich weiß nicht, wie lange genau - hatte ich wenig oder gar kein Geld in der Tasche und nichts Besonderes, was mich an Land interessierte, und so dachte ich, ich würde ein wenig herumsegeln und den wässrigen Teil der Welt sehen.\n",
      "\n",
      "Несколько лет назад - неважно, как давно, - когда в моем кошельке почти не было денег, а на берегу не было ничего особенного, что могло бы меня заинтересовать, я решил немного поплавать и посмотреть водную часть мира.\n"
     ]
    }
   ],
   "source": [
    "print(de_text[1])\n",
    "print()\n",
    "print(ru_text[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasorb translations\n",
    "\n",
    "data =[]\n",
    "\n",
    "for bridge in bridges:\n",
    "    origin, target = bridge\n",
    "    sub = []\n",
    "    for item in origin:\n",
    "        if type(item)==list:\n",
    "            sub.append(item)\n",
    "\n",
    "    data.append(sub)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for t in translations:\n",
    "\n",
    "    t=  t.replace(\".]\",\"]\")\n",
    "    t = t.replace(\"]]]\",\"]]\")\n",
    "    t = t.replace(\"]]]\",\"]]\")\n",
    "\n",
    "    if not t.endswith(\"]]\"):\n",
    "        t = t[:t.rfind(\"]\")+1]\n",
    "    if not t.endswith(\"]]\"):\n",
    "        t = t + \"]\"\n",
    "\n",
    "    if not t.endswith(\"\\\"]]\"):\n",
    "        t.replace(\"]]\", \"\\\"]\")\n",
    "\n",
    "    data_point = []\n",
    "    try:\n",
    "        data_point = (json.loads(t))\n",
    "    except Exception as e:\n",
    "        print(e,t)\n",
    "        # try:\n",
    "        #     data_point = (json.loads(t[:t.rfind(\"]\")+1]+\"]\"))\n",
    "        # except Exception as e:\n",
    "\n",
    "        #     print(\"final try run \",e,t)\n",
    "\n",
    "\n",
    "    data_point = filter(lambda x: len(x)==2, data_point)\n",
    "    data.append(list(data_point))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Milz', 'dalağ'],\n",
       " ['vertreiben', 'kurtulmak'],\n",
       " ['Kreislauf', 'kan dolaşımı'],\n",
       " ['regulieren', 'düzenlemek']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2word :\n",
    "    \n",
    "    def __init__(self, from_language:str, to_language:str):\n",
    "        self.from_language = from_language\n",
    "        self.to_language = to_language\n",
    "        self.data:dict[str,set[str]] = {}\n",
    "        self.anti:dict[str,set[str]] = {}\n",
    "\n",
    "    def add_translation(self, from_word:str, to_word:str):\n",
    "\n",
    "        if from_word not in self.data:\n",
    "            self.data[from_word] = {to_word}\n",
    "        else:\n",
    "            self.data[from_word].add(to_word)\n",
    "\n",
    "        if to_word not in self.anti:\n",
    "            self.anti[to_word] = {from_word}\n",
    "        else:\n",
    "            self.anti[to_word].add(from_word)\n",
    "\n",
    "    def get_translations(self, from_word:str):\n",
    "        if from_word in self.data:\n",
    "            return self.data[from_word]\n",
    "        else:\n",
    "            return set()\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"../static/translations/{self.from_language}_{self.to_language}.json\", \"w\") as f:\n",
    "            data_obj = {}\n",
    "            for k,v in self.data.items():\n",
    "                data_obj[k] = list(v)\n",
    "\n",
    "            json.dump(data_obj, f,indent=4)\n",
    "\n",
    "        with open(f\"../static/translations/{self.to_language}_{self.from_language}.json\", \"w\") as f:\n",
    "            data_obj = {}\n",
    "            for k,v in self.anti.items():\n",
    "                data_obj[k] = list(v)\n",
    "\n",
    "            json.dump(data_obj, f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "de2en = Word2word(\"de\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    for t in d:\n",
    "        de2en.add_translation(t[0],t[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2WordCollection:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    \n",
    "    def add(self, vocab: tuple[str,str], lang:str = \"de2en\"):\n",
    "        \n",
    "        origin,target =  lang.split(\"2\")\n",
    "\n",
    "        anti_lang = target + \"2\" + origin\n",
    "        anti_vocab = (vocab[1],vocab[0])\n",
    "\n",
    "        self._add(vocab,lang)\n",
    "        self._add(anti_vocab,anti_lang)\n",
    "\n",
    "    \n",
    "    def _add(self,vocab,lang:str):\n",
    "\n",
    "        if lang not in self.data:\n",
    "            self.data[lang] = {}\n",
    "        \n",
    "        item = self.data[lang]\n",
    "        if vocab[0] not in item:\n",
    "            item[vocab[0]] = [vocab[1]]\n",
    "        else:\n",
    "            item[vocab[0]].append(vocab[1])\n",
    "\n",
    "    def save (self):\n",
    "        for lang, vocab in self.data.items():\n",
    "            with open(f\"../static/translations/{lang}.json\", \"w\") as f:\n",
    "                json.dump(vocab, f,indent=4)\n",
    "\n",
    "\n",
    "w2wc = Word2WordCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data: \n",
    "    for v in d:\n",
    "        w2wc.add(v,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2wc.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bridge  = list[list[list[(str|list[str])]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Vor einigen Jahren - ich weiß nicht, wie lange genau - hatte ich ',\n",
       "  ['az', 'wenig'],\n",
       "  ' oder ',\n",
       "  ['hiç', 'gar kein'],\n",
       "  ' ',\n",
       "  ['para', 'Geld'],\n",
       "  ' in der ',\n",
       "  ['cüzdan', 'Tasche'],\n",
       "  ' und nichts ',\n",
       "  ['özel', 'Besonderes'],\n",
       "  ', was mich ',\n",
       "  ['karada', 'an Land'],\n",
       "  ' interessierte, und so dachte ich, ich würde ein ',\n",
       "  ['az', 'wenig'],\n",
       "  ' ',\n",
       "  ['yelken açmak', 'herumsegeln'],\n",
       "  ' und den ',\n",
       "  ['sulu', 'wässrigen'],\n",
       "  ' ',\n",
       "  ['kısım', 'Teil'],\n",
       "  ' der ',\n",
       "  ['dünya', 'Welt'],\n",
       "  ' ',\n",
       "  ['görmeyi düşünmek', 'sehen'],\n",
       "  '.'],\n",
       " ['Birkaç ',\n",
       "  ['Jahre', 'yıl'],\n",
       "  ' önce -tam olarak ne kadar zaman olduğu önemli değil- cüzdanımda çok ',\n",
       "  ['wenig', 'az'],\n",
       "  ' ',\n",
       "  ['Geld', 'para'],\n",
       "  ' olduğu ya da ',\n",
       "  ['gar kein', 'hiç'],\n",
       "  ' ',\n",
       "  ['Geld', 'para'],\n",
       "  ' olmadığı ve kıyıda ilgimi çekecek ',\n",
       "  ['Besonderes', 'özel'],\n",
       "  ' bir şey olmadığı için biraz yelken açıp dünyanın su kısmını görmeyi düşündüm.']]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bridge(origins:list[str], translations:list[str], translatorlist:list[list[list[str]]])->Bridge:\n",
    "    res = []\n",
    "    for origin, translation, translator in zip(origins, translations,translatorlist):\n",
    "\n",
    "        origin_data = [origin]\n",
    "        translation_data = [translation]\n",
    "        \n",
    "        for a,b in translator:\n",
    "\n",
    "            for d in origin_data:\n",
    "                if type (d) == str:\n",
    "                    insert_translations(d,a,b,origin_data)\n",
    "\n",
    "            for d in translation_data:\n",
    "                if type (d) == str:\n",
    "                    insert_translations(d,b,a,translation_data)\n",
    "\n",
    "        res.append([origin_data, translation_data])\n",
    "\n",
    "    return res\n",
    "\n",
    "def insert_translations(sentence:str, origin:str, target:str, origin_data):\n",
    "    pattern = r'\\b' + re.escape(origin.lower()) + r'\\b'\n",
    "\n",
    "    matches = re.finditer(pattern, sentence.lower())\n",
    "    for match in matches:\n",
    "        pre = sentence[:match.start()]\n",
    "        core = [target,sentence[match.start():match.end()]]\n",
    "        post = sentence[match.end():]\n",
    "        idx = origin_data.index(sentence)\n",
    "        origin_data.remove(sentence)\n",
    "        origin_data.insert(idx,pre)\n",
    "        origin_data.insert(idx+1,core)\n",
    "        origin_data.insert(idx+2,post)\n",
    "        break\n",
    "\n",
    "# make_bridge([\"Vor einigen Jahren sah er mich\"], [\"Some years ago he saw me\"], [[[\"einigen\", \"Some\"],[\"mich\",\"me\"]]])\n",
    "\n",
    "i = 2\n",
    "make_bridge (origin_text[:i],target_text[:i],data[:i])[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = make_bridge (origin_text,target_text,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f\"../static/books/mobydick/{lang}.json\",\"w\") as f:\n",
    "    json.dump(bridge, f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
