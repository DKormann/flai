{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prerequisite:\n",
    "\n",
    "`%pip install word2word`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = \"de2es\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanup(name:str,lang:str=\"en\"):\n",
    "\n",
    "    raw_content = open(f\"../static/books/{name}/{lang}.raw\", \"r\").read()\n",
    "\n",
    "    raw_content = raw_content.replace(\"\\n\",\" \")\n",
    "    raw_content = raw_content.replace(\"Mr.\",\"Mr<dot>\")\n",
    "    raw_content = raw_content.replace(\"Mrs.\",\"Mrs<dot>\")\n",
    "    raw_content = raw_content.replace(\". .\",\"..\")\n",
    "    raw_content = raw_content.replace(\". .\",\"..\")\n",
    "    raw_content = raw_content.replace(\"...\",\"…\")\n",
    "\n",
    "    raw_content = raw_content.replace(\". \",\".\\n\")\n",
    "    raw_content = raw_content.replace(\"? \",\"?\\n\")\n",
    "    raw_content = raw_content.replace(\"! \",\"!\\n\")\n",
    "    raw_content = raw_content.replace(\"\\n\\n\",\"\\n\")\n",
    "\n",
    "    raw_content = raw_content.replace(\"\\n\\\"\\n\",\"\\\"\\n\")\n",
    "\n",
    "\n",
    "    raw_content = raw_content.replace(\"<dot>\",\".\")\n",
    "\n",
    "    lines = raw_content.split(\"\\n\")\n",
    "\n",
    "    lines = map(lambda x: x.strip(), lines)\n",
    "    lines = map(lambda x: x.strip(\"\\n\"), lines)\n",
    "    \n",
    "    parts = [\"\"]\n",
    "\n",
    "    min_content = 100\n",
    "\n",
    "    max_content = 200\n",
    "\n",
    "    # for line in lines:\n",
    "    #     if len (parts[-1]) < min_content and len(parts[-1] + line) < max_content: \n",
    "    #         parts[-1] += \"\\n\" + line\n",
    "    #     else:\n",
    "    #         parts.append(line)\n",
    "\n",
    "    parts = list(lines)\n",
    "\n",
    "    parts[0] = parts[0].strip(\"\\n\")\n",
    "    \n",
    "    print(\"num parts\",len(parts))\n",
    "    # longest part \n",
    "    print(\"longest part\",max([len(part) for part in parts]))\n",
    "\n",
    "\n",
    "    # json.dump(parts, open(f\"../static/books/{name}/{lang}.json\", \"w\"),indent=4)\n",
    "\n",
    "    with open(f\"../static/books/{name}/{lang}.lines\", \"w\") as outfile:\n",
    "        for line in parts:\n",
    "            outfile.write(line + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# cleanup(\"the_little_prince\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../static/books/mobydick/es.raw'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cleanup(\u001b[39m\"\u001b[39;49m\u001b[39mmobydick\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m, in \u001b[0;36mcleanup\u001b[0;34m(name, lang)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcleanup\u001b[39m(name:\u001b[39mstr\u001b[39m,lang:\u001b[39mstr\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m----> 3\u001b[0m     raw_content \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m../static/books/\u001b[39;49m\u001b[39m{\u001b[39;49;00mname\u001b[39m}\u001b[39;49;00m\u001b[39m/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlang\u001b[39m}\u001b[39;49;00m\u001b[39m.raw\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39mread()\n\u001b[1;32m      5\u001b[0m     raw_content \u001b[39m=\u001b[39m raw_content\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m     raw_content \u001b[39m=\u001b[39m raw_content\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mMr.\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mMr<dot>\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:282\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    276\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    277\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    279\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m     )\n\u001b[0;32m--> 282\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../static/books/mobydick/es.raw'"
     ]
    }
   ],
   "source": [
    "cleanup(\"mobydick\",\"es\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## book collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BookCollection:\n",
    "    def __init__(self, data:dict[str,dict[str,list[str]]] = {}):\n",
    "        self.data = data\n",
    "    \n",
    "    def add_book(self, lang:str,title:str, txt:list[str]):\n",
    "        if lang in self.data:\n",
    "            self.data[lang][title] = txt\n",
    "        else:\n",
    "            self.data[lang] = {title:txt}\n",
    "\n",
    "    def get_book(self, lang:str, title:str):\n",
    "\n",
    "        if lang in self.data and title in self.data[lang]:\n",
    "            return self.data[lang][title]\n",
    "\n",
    "        else:\n",
    "            \n",
    "            dir = f\"../static/books/{title}/{lang}.lines\"\n",
    "            if os.path.exists(dir):\n",
    "                book = []\n",
    "                for line in open(dir,\"r\"):\n",
    "                    line = line.strip()\n",
    "                    if len(line) > 0:\n",
    "                        book.append(line)\n",
    "                self.add_book(lang,title,book)\n",
    "                return book\n",
    "            else:\n",
    "                print(\"no such book\",dir)\n",
    "                return None\n",
    "    def remove_book (self, lang:str, title:str):\n",
    "        if lang in self.data and title in self.data[lang]:\n",
    "            del self.data[lang][title]\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "bookcollection = BookCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_text = bookcollection.get_book(\"de\",\"mobydick\")\n",
    "target_text = bookcollection.get_book(\"es\",\"mobydick\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## brige"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompts = {\n",
    "    \"de2ru\": '''Ich versuche russisch zu lernen, indem ich ein Buch lese. Der Originaltext lautet [{}] die deutsche Version lautet [{}]\n",
    "finde alle deutschen Wörter und die passende russische Übersetzung aus dem Text und gib sie im JSON-Format an, wie hier: [[\"Apfel\",\"яблоко\"],[\"essen\", \"есть\"]]\n",
    "KI: Hier ist eine Liste von Übersetzungen im JSON-Format: [[\"''',\n",
    "\n",
    "    \"de2tr\": '''Ich versuche türkisch zu lernen, indem ich ein Buch lese. Der Originaltext lautet [{}] die deutsche Version lautet [{}]\n",
    "finde alle deutschen Wörter und die passende türkische Übersetzung aus dem Text und gib sie im JSON-Format an, wie hier: [[\"Apfel\",\"elma\"],[\"essen\", \"yemek\"]]\n",
    "KI: Hier ist eine Liste von Übersetzungen im JSON-Format: [[\"''',\n",
    "\n",
    "    \"de2es\": '''Estoy intentando aprender español leyendo un libro. El texto original es [{}] la versión alemana es [{}]\n",
    "encuentra todas las palabras alemanas y la traducción española correspondiente del texto y ponlas en formato JSON como aquí: [[\"Apfel\",\"manzana\"],[\"essen\", \"comer\"]]\n",
    "IA: Aquí hay una lista de traducciones en formato JSON: [[\"''',\n",
    "\n",
    "    \"en2de\": '''I am trying to learn german by reading a book. the original text is [{}] the german version is [{}]\n",
    "translate the german words and put the translations for single words in json format like this: [[\"apple\", \"Apfel\"],[\"eat\", \"essen\"]]\n",
    "AI: here is a list of translations: [[\"'''\n",
    "}\n",
    "\n",
    "def get_translations(origin_sentence,translation_sentence,lang=\"en2de\"):\n",
    "\n",
    "    prompt = prompts[lang].format(origin_sentence,translation_sentence)\n",
    "\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model = \"gpt-3.5-turbo\",\n",
    "        messages= [{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens = 500,\n",
    "        stop = \"]]\"\n",
    "    )\n",
    "\n",
    "    txt = resp.choices[0].message.content\n",
    "    answer = '[[\"'+txt + \"]]\"\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vor einigen Jahren - ich weiß nicht, wie lange genau - hatte ich wenig oder gar kein Geld in der Tasche und nichts Besonderes, was mich an Land interessierte, und so dachte ich, ich würde ein wenig herumsegeln und den wässrigen Teil der Welt sehen.\n",
      "\n",
      "Hace algunos años -no sé cuánto tiempo exactamente-, con poco o nada de dinero en mi monedero y sin nada que me interesara en tierra, pensé en navegar un poco y ver la parte acuática del mundo.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[[\"Jahre\",\"años\"],[\"nicht\",\"no\"],[\"wie\",\"cómo\"],[\"lange\",\"tiempo\"],[\"genau\",\"exactamente\"],[\"wenig\",\"poco\"],[\"oder\",\"o\"],[\"gar\",\"nada\"],[\"Geld\",\"dinero\"],[\"Tasche\",\"monedero\"],[\"nichts\",\"nada\"],[\"Besonderes\",\"interesara\"],[\"Land\",\"tierra\"],[\"so\",\"así\"],[\"dachte\",\"pensé\"],[\"ein\",\"un\"],[\"herumsegeln\",\"navegar\"],[\"wässrigen\",\"acuática\"],[\"Teil\",\"parte\"],[\"Welt\",\"mundo\"]]'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "i=1\n",
    "# get_translations(en_text[i],de_text[i])\n",
    "\n",
    "print(origin_text[i])\n",
    "print()\n",
    "print(target_text[i])\n",
    "print()\n",
    "\n",
    "get_translations(origin_text[i], target_text[i],lang=lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "translating 100 sentences\n",
      "..........9\n",
      ".....That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 03a08e68d292613ec74561278a475d7d in your message.)\n",
      ".....19\n",
      "..........29\n",
      "....That model is currently overloaded with other requests. You can retry your request, or contact us through our help center at help.openai.com if the error persists. (Please include the request ID 54a03bce54e2047b4cc0b09bb4a01d12 in your message.)\n",
      "......39\n",
      "..........49\n",
      "..........59\n",
      "..........69\n",
      "..........79\n",
      "..........89\n",
      "..........99\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "n = len(origin_text)\n",
    "\n",
    "print(f\"translating {n} sentences\")\n",
    "for i in range(len(translations),n):\n",
    "\n",
    "    params = [origin_text[i],target_text[i],lang]\n",
    "    try:\n",
    "        new_t = get_translations(*params)\n",
    "        translations.append(new_t)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        \n",
    "        # retry once\n",
    "        try:\n",
    "            new_t = get_translations(*params)\n",
    "            translations.append(new_t)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            translations.append(\"<error>\")\n",
    "    \n",
    "    print(\".\",end=\"\")\n",
    "    if (i+1) % 10 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reasorb translations\n",
    "\n",
    "data =[]\n",
    "\n",
    "for bridge in bridges:\n",
    "    origin, target = bridge\n",
    "    sub = []\n",
    "    for item in origin:\n",
    "        if type(item)==list:\n",
    "            sub.append(item)\n",
    "\n",
    "    data.append(sub)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line 42 Unterminated string starting at: line 1 column 1253 (char 1252) [[\"Besuchen\", \"visitar\"], [\"die\", \"el/la/los/las\"], [\"Prärien\", \"praderas\"], [\"im\", \"en/el/la/los/las\"], [\"Juni\", \"junio\"], [\"wenn\", \"cuando\"], [\"Sie\", \"usted\"], [\"Dutzende\", \"docenas\"], [\"von\", \"de\"], [\"Kilometern\", \"kilómetros\"], [\"knietief\", \"hasta las rodillas\"], [\"zwischen\", \"entre\"], [\"Tigerlilien\", \"lirios tigre\"], [\"waten\", \"vadear\"], [\"-\",\"-\"], [\"was\", \"qué\"], [\"ist\", \"es\"], [\"der\", \"el/la\"], [\"einzige\", \"único\"], [\"Zauber\", \"encanto\"], [\"der\", \"el/la\"], [\"fehlt\", \"falta\"], [\"?\", \"?\"], [\"-\", \"-\"], [\"Wasser\", \"agua\"], [\"-\", \"-\"], [\"es\", \"allí\"], [\"gibt\", \"hay\"], [\"dort\", \"allí\"], [\"keinen\", \"ningún\"], [\"Tropfen\", \"gota\"], [\"Wasser\", \"agua\"], [\"!\", \"!\"], [\"Pero\", \"Pero\"], [\"aunque\", \"aunque\"], [\"la\", \"el/la\"], [\"imagen\", \"imagen\"], [\"yace\", \"yace\"], [\"así\", \"así\"], [\"en\", \"en\"], [\"trance\", \"trance\"], [\",\", \",\"], [\"y\", \"y\"], [\"este\", \"este\"], [\"pino\", \"pino\"], [\"sacude\", \"sacude\"], [\"sus\", \"sus\"], [\"suspiros\", \"suspiros\"], [\"como\", \"como\"], [\"hojas\", \"hojas\"], [\"sobre\", \"sobre\"], [\"la\", \"la\"], [\"cabeza\", \"cabeza\"], [\"de\", \"de\"], [\"este\", \"este\"], [\"pastor\", \"pastor\"], [\",\", \",\"], [\"todo\", \"todo\"], [\"es\", \"es\"], [\"en\", \"en\"], [\"vano\", \"vano\"], [\",\", \",\"], [\"a\", \"a\"], [\"menos\", \"menos\"], [\"que\", \"que\"], [\"el\", \"el/la\"], [\"ojo\", \"o]]\n",
      "line 74 Expecting value: line 1 column 1261 (char 1260) [[\"Ich\", \"yo\"], [\"fahre\", \"voy\"], [\"immer\", \"siempre\"], [\"als\", \"como\"], [\"Seemann\", \"marinero\"], [\"zur\", \"a la\"], [\"See\", \"mar\"], [\"weil\", \"porque\"], [\"sie\", \"ellos\"], [\"mich\", \"a mí\"], [\"für\", \"por\"], [\"meine\", \"mi\"], [\"Mühe\", \"esfuerzo\"], [\"bezahlen\", \"pagar\"], [\"während\", \"mientras\"], [\"den\", \"el\"], [\"Passagieren\", \"pasajeros\"], [\"keinen\", \"ningún\"], [\"einzigen\", \"único\"], [\"Pfennig\", \"penique\"], [\"gezahlt\", \"pagado\"], [\"habe\", \"he\"],[\"Bien\", \"bien\"], [\"entonces\", \"entonces\"], [\"por\", \"por\"], [\"más\", \"más\"], [\"los\", \"los\"], [\"viejos\", \"viejos\"], [\"capitanes\", \"capitanes\"], [\"de\", \"del\"], [\"mar\", \"mar\"], [\"me\", \"me\"], [\"den\", \"dan\"], [\"órdenes\", \"órdenes\"], [\"golpeen\", \"golpeen\"], [\"y\", \"y\"], [\"puñetazos\", \"puñetazos\"], [\"tengo\", \"tengo\"], [\"la\", \"la\"], [\"satisfacción\", \"satisfacción\"], [\"saber\", \"saber\"], [\"todo\", \"todo\"], [\"está\", \"está\"], [\"bien\", \"bien\"], [\"todos\", \"todos\"], [\"demás\", \"los demás\"], [\"son\", \"son\"], [\"servidos\", \"servidos\"], [\"u\", \"o\"], [\"otra\", \"otra\"], [\"manera\", \"manera\"], [\"desde\", \"desde\"], [\"punto\", \"punto\"], [\"vista\", \"vista\"], [\"físico\", \"físico\"], [\"o\", \"o\"], [\"metafísico\", \"metafísico\"], [\"y\", \"y\"], [\"así\", \"así\"], [\"golpe\", \"golpe\"], [\"universal\", \"universal\"], [\"se\", \"se\"], [\"reparte\", \"reparte\"], [\"deben\",]]\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for i,t in enumerate(translations):\n",
    "\n",
    "    t=  t.replace(\".]\",\"]\")\n",
    "    t = t.replace(\"]]]\",\"]]\")\n",
    "    t = t.replace(\"]]]\",\"]]\")\n",
    "\n",
    "    if not t.endswith(\"]]\"):\n",
    "        t = t[:t.rfind(\"]\")+1]\n",
    "    if not t.endswith(\"]]\"):\n",
    "        t = t + \"]\"\n",
    "\n",
    "    if not t.endswith(\"\\\"]]\"):\n",
    "        t.replace(\"]]\", \"\\\"]\")\n",
    "\n",
    "    data_point = []\n",
    "    try:\n",
    "        data_point = (json.loads(t))\n",
    "    except Exception as e:\n",
    "        print(\"line\",i,e,t)\n",
    "        # try:\n",
    "        #     data_point = (json.loads(t[:t.rfind(\"]\")+1]+\"]\"))\n",
    "        # except Exception as e:\n",
    "\n",
    "        #     print(\"final try run \",e,t)\n",
    "\n",
    "\n",
    "    data_point = filter(lambda x: len(x)==2, data_point)\n",
    "    data.append(list(data_point))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Weise', 'manera'],\n",
       " ['Milz', 'bazo'],\n",
       " ['vertreiben', 'ahuyentar'],\n",
       " ['Kreislauf', 'circulación'],\n",
       " ['regulieren', 'regular']]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.count([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2word :\n",
    "    \n",
    "    def __init__(self, from_language:str, to_language:str):\n",
    "        self.from_language = from_language\n",
    "        self.to_language = to_language\n",
    "        self.data:dict[str,set[str]] = {}\n",
    "        self.anti:dict[str,set[str]] = {}\n",
    "\n",
    "    def add_translation(self, from_word:str, to_word:str):\n",
    "\n",
    "        if from_word not in self.data:\n",
    "            self.data[from_word] = {to_word}\n",
    "        else:\n",
    "            self.data[from_word].add(to_word)\n",
    "\n",
    "        if to_word not in self.anti:\n",
    "            self.anti[to_word] = {from_word}\n",
    "        else:\n",
    "            self.anti[to_word].add(from_word)\n",
    "\n",
    "    def get_translations(self, from_word:str):\n",
    "        if from_word in self.data:\n",
    "            return self.data[from_word]\n",
    "        else:\n",
    "            return set()\n",
    "\n",
    "    def save(self):\n",
    "        with open(f\"../static/translations/{self.from_language}_{self.to_language}.json\", \"w\") as f:\n",
    "            data_obj = {}\n",
    "            for k,v in self.data.items():\n",
    "                data_obj[k] = list(v)\n",
    "\n",
    "            json.dump(data_obj, f,indent=4)\n",
    "\n",
    "        with open(f\"../static/translations/{self.to_language}_{self.from_language}.json\", \"w\") as f:\n",
    "            data_obj = {}\n",
    "            for k,v in self.anti.items():\n",
    "                data_obj[k] = list(v)\n",
    "\n",
    "            json.dump(data_obj, f,indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Word2word' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m de2en \u001b[39m=\u001b[39m Word2word(\u001b[39m\"\u001b[39m\u001b[39mde\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Word2word' is not defined"
     ]
    }
   ],
   "source": [
    "de2en = Word2word(\"de\",\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data:\n",
    "    for t in d:\n",
    "        de2en.add_translation(t[0],t[1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2WordCollection:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "    \n",
    "    def add(self, vocab: tuple[str,str], lang:str = \"de2en\"):\n",
    "        \n",
    "        origin,target =  lang.split(\"2\")\n",
    "\n",
    "        anti_lang = target + \"2\" + origin\n",
    "        anti_vocab = (vocab[1],vocab[0])\n",
    "\n",
    "        self._add(vocab,lang)\n",
    "        self._add(anti_vocab,anti_lang)\n",
    "\n",
    "    \n",
    "    def _add(self,vocab,lang:str):\n",
    "\n",
    "        if lang not in self.data:\n",
    "            self.data[lang] = {}\n",
    "        \n",
    "        item = self.data[lang]\n",
    "        if vocab[0] not in item:\n",
    "            item[vocab[0]] = [vocab[1]]\n",
    "        else:\n",
    "            item[vocab[0]].append(vocab[1])\n",
    "\n",
    "    def save (self):\n",
    "        for lang, vocab in self.data.items():\n",
    "            with open(f\"../static/translations/{lang}.json\", \"w\") as f:\n",
    "                json.dump(vocab, f,indent=4)\n",
    "\n",
    "\n",
    "w2wc = Word2WordCollection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data: \n",
    "    for v in d:\n",
    "        w2wc.add(v,lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2wc.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bridge  = list[list[list[(str|list[str])]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Vor ',\n",
       "  ['algunos años', 'einigen Jahren'],\n",
       "  ' - ',\n",
       "  ['yo', 'ich'],\n",
       "  ' ',\n",
       "  ['no sé', 'weiß nicht'],\n",
       "  ', ',\n",
       "  ['cuánto tiempo', 'wie lange'],\n",
       "  ' ',\n",
       "  ['exactamente', 'genau'],\n",
       "  ' - ',\n",
       "  ['tenía', 'hatte'],\n",
       "  ' ',\n",
       "  ['yo', 'ich'],\n",
       "  ' ',\n",
       "  ['poco', 'wenig'],\n",
       "  ' ',\n",
       "  ['o', 'oder'],\n",
       "  ' ',\n",
       "  ['nada', 'gar'],\n",
       "  ' ',\n",
       "  ['sin dinero', 'kein Geld'],\n",
       "  ' ',\n",
       "  ['en mi monedero', 'in der Tasche'],\n",
       "  ' und ',\n",
       "  ['nada que me interesara', 'nichts Besonderes'],\n",
       "  ', ',\n",
       "  ['que me', 'was mich'],\n",
       "  ' ',\n",
       "  ['en tierra', 'an Land'],\n",
       "  ' ',\n",
       "  ['interesara', 'interessierte'],\n",
       "  ', und ',\n",
       "  ['así', 'so'],\n",
       "  ' dachte ',\n",
       "  ['yo', 'ich'],\n",
       "  ', ',\n",
       "  ['yo', 'ich'],\n",
       "  ' würde ein ',\n",
       "  ['poco', 'wenig'],\n",
       "  ' ',\n",
       "  ['navegar un poco', 'herumsegeln'],\n",
       "  ' und ',\n",
       "  ['la parte acuática', 'den wässrigen Teil'],\n",
       "  ' ',\n",
       "  ['del mundo', 'der Welt'],\n",
       "  ' sehen.'],\n",
       " ['Hace ',\n",
       "  ['einigen Jahren', 'algunos años'],\n",
       "  ' -',\n",
       "  ['weiß nicht', 'no sé'],\n",
       "  ' ',\n",
       "  ['wie lange', 'cuánto tiempo'],\n",
       "  ' ',\n",
       "  ['genau', 'exactamente'],\n",
       "  '-, con ',\n",
       "  ['wenig', 'poco'],\n",
       "  ' ',\n",
       "  ['oder', 'o'],\n",
       "  ' ',\n",
       "  ['gar', 'nada'],\n",
       "  ' de dinero ',\n",
       "  ['in der Tasche', 'en mi monedero'],\n",
       "  ' y sin ',\n",
       "  ['gar', 'nada'],\n",
       "  ' ',\n",
       "  ['was mich', 'que me'],\n",
       "  ' ',\n",
       "  ['interessierte', 'interesara'],\n",
       "  ' ',\n",
       "  ['an Land', 'en tierra'],\n",
       "  ', ',\n",
       "  ['dachte ich', 'pensé en'],\n",
       "  ' navegar un ',\n",
       "  ['wenig', 'poco'],\n",
       "  ' y ver ',\n",
       "  ['den wässrigen Teil', 'la parte acuática'],\n",
       "  ' ',\n",
       "  ['der Welt', 'del mundo'],\n",
       "  '.']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_bridge(origins:list[str], translations:list[str], translatorlist:list[list[list[str]]])->Bridge:\n",
    "    res = []\n",
    "    for origin, translation, translator in zip(origins, translations,translatorlist):\n",
    "\n",
    "        origin_data = [origin]\n",
    "        translation_data = [translation]\n",
    "        \n",
    "        for a,b in translator:\n",
    "\n",
    "            for d in origin_data:\n",
    "                if type (d) == str:\n",
    "                    insert_translations(d,a,b,origin_data)\n",
    "\n",
    "            for d in translation_data:\n",
    "                if type (d) == str:\n",
    "                    insert_translations(d,b,a,translation_data)\n",
    "\n",
    "        res.append([origin_data, translation_data])\n",
    "\n",
    "    return res\n",
    "\n",
    "def insert_translations(sentence:str, origin:str, target:str, origin_data):\n",
    "    pattern = r'\\b' + re.escape(origin.lower()) + r'\\b'\n",
    "\n",
    "    matches = re.finditer(pattern, sentence.lower())\n",
    "    for match in matches:\n",
    "        pre = sentence[:match.start()]\n",
    "        core = [target,sentence[match.start():match.end()]]\n",
    "        post = sentence[match.end():]\n",
    "        idx = origin_data.index(sentence)\n",
    "        origin_data.remove(sentence)\n",
    "        origin_data.insert(idx,pre)\n",
    "        origin_data.insert(idx+1,core)\n",
    "        origin_data.insert(idx+2,post)\n",
    "        break\n",
    "\n",
    "# make_bridge([\"Vor einigen Jahren sah er mich\"], [\"Some years ago he saw me\"], [[[\"einigen\", \"Some\"],[\"mich\",\"me\"]]])\n",
    "\n",
    "i = 2\n",
    "make_bridge (origin_text[:i],target_text[:i],data[:i])[i-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "bridge = make_bridge (origin_text,target_text,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (f\"../static/books/mobydick/{lang}.json\",\"w\") as f:\n",
    "    json.dump(bridge, f,indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
